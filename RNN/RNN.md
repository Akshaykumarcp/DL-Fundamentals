### Sequential Dependence
- Stock prices
- Weather
- Text input

### Traditional NN Issues
- No tracking of data sequences
- Do not remember previous values
- RNNs address these issues

### RNN features
- Sequential information
- Same computations
- Uses current and previous data

### RNN Training Issues
- Back Propagation issues
- Exploding  Gradient
- Vanishing Gradient
- Fix training issues

### LSTMs and GRUs
- Cell State memory
- Accessed via Gates
- http://colah.github.io/posts/2015-08-Understanding-LSTMs

### Gated Recurrent Unit(GRU)
- Simplifies LSTM
- Forget + Input = Update
- Merges Cell State and Hidden State

### RNN issues
- Determining meaning is hard